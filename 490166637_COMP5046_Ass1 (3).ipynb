{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGHoy6KpQDfZ"
   },
   "source": [
    "# Unikey# 490166637 \n",
    "COMP5046 Assignment 1\n",
    "*Make sure you change the file name with your unikey.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTf21j_oQIiD"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the user, please mention here.* \n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXbQohXLKSgO"
   },
   "source": [
    "***Visualising the comparison of different results is a good way to justify your decision.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34DVNKgqQY21"
   },
   "source": [
    "# 1 - Data Preprocessing (Personality chat datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cWUxAQrGlq6"
   },
   "source": [
    "## 1.1. Download Dataset (Personality chat datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "QtBCpTykY4OH",
    "outputId": "66e479a7-5c27-45f8-e734-80e66300f16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import gzip\n",
    "import gensim \n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from scipy import spatial\n",
    "from collections import OrderedDict\n",
    "from google.colab import files\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "U7C4snIcNl22",
    "outputId": "3e9a10ca-a0ed-471c-d4a8-4d7bbb38bd21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K    1% |▎                               | 10kB 18.2MB/s eta 0:00:01\r",
      "\u001b[K    2% |▋                               | 20kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K    3% |█                               | 30kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K    4% |█▎                              | 40kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K    5% |█▋                              | 51kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K    6% |██                              | 61kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K    7% |██▎                             | 71kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K    8% |██▋                             | 81kB 5.7MB/s eta 0:00:01\r",
      "\u001b[K    9% |███                             | 92kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K    10% |███▎                            | 102kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K    11% |███▋                            | 112kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K    12% |████                            | 122kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K    13% |████▎                           | 133kB 6.9MB/s eta 0:00:01\r",
      "\u001b[K    14% |████▋                           | 143kB 12.6MB/s eta 0:00:01\r",
      "\u001b[K    15% |█████                           | 153kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K    16% |█████▎                          | 163kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K    17% |█████▋                          | 174kB 13.3MB/s eta 0:00:01\r",
      "\u001b[K    18% |██████                          | 184kB 13.7MB/s eta 0:00:01\r",
      "\u001b[K    19% |██████▎                         | 194kB 49.2MB/s eta 0:00:01\r",
      "\u001b[K    20% |██████▋                         | 204kB 43.0MB/s eta 0:00:01\r",
      "\u001b[K    21% |███████                         | 215kB 15.0MB/s eta 0:00:01\r",
      "\u001b[K    22% |███████▎                        | 225kB 14.9MB/s eta 0:00:01\r",
      "\u001b[K    23% |███████▋                        | 235kB 15.3MB/s eta 0:00:01\r",
      "\u001b[K    24% |████████                        | 245kB 15.4MB/s eta 0:00:01\r",
      "\u001b[K    25% |████████▎                       | 256kB 15.4MB/s eta 0:00:01\r",
      "\u001b[K    26% |████████▋                       | 266kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K    27% |█████████                       | 276kB 14.8MB/s eta 0:00:01\r",
      "\u001b[K    29% |█████████▎                      | 286kB 14.8MB/s eta 0:00:01\r",
      "\u001b[K    30% |█████████▋                      | 296kB 14.8MB/s eta 0:00:01\r",
      "\u001b[K    31% |██████████                      | 307kB 15.7MB/s eta 0:00:01\r",
      "\u001b[K    32% |██████████▎                     | 317kB 53.0MB/s eta 0:00:01\r",
      "\u001b[K    33% |██████████▋                     | 327kB 54.0MB/s eta 0:00:01\r",
      "\u001b[K    34% |███████████                     | 337kB 55.8MB/s eta 0:00:01\r",
      "\u001b[K    35% |███████████▎                    | 348kB 48.3MB/s eta 0:00:01\r",
      "\u001b[K    36% |███████████▋                    | 358kB 48.2MB/s eta 0:00:01\r",
      "\u001b[K    37% |████████████                    | 368kB 59.5MB/s eta 0:00:01\r",
      "\u001b[K    38% |████████████▎                   | 378kB 60.0MB/s eta 0:00:01\r",
      "\u001b[K    39% |████████████▋                   | 389kB 60.5MB/s eta 0:00:01\r",
      "\u001b[K    40% |█████████████                   | 399kB 21.0MB/s eta 0:00:01\r",
      "\u001b[K    41% |█████████████▎                  | 409kB 17.1MB/s eta 0:00:01\r",
      "\u001b[K    42% |█████████████▋                  | 419kB 17.0MB/s eta 0:00:01\r",
      "\u001b[K    43% |██████████████                  | 430kB 16.8MB/s eta 0:00:01\r",
      "\u001b[K    44% |██████████████▎                 | 440kB 16.7MB/s eta 0:00:01\r",
      "\u001b[K    45% |██████████████▋                 | 450kB 16.8MB/s eta 0:00:01\r",
      "\u001b[K    46% |███████████████                 | 460kB 16.7MB/s eta 0:00:01\r",
      "\u001b[K    47% |███████████████▎                | 471kB 16.7MB/s eta 0:00:01\r",
      "\u001b[K    48% |███████████████▋                | 481kB 16.7MB/s eta 0:00:01\r",
      "\u001b[K    49% |████████████████                | 491kB 16.7MB/s eta 0:00:01\r",
      "\u001b[K    50% |████████████████▎               | 501kB 35.1MB/s eta 0:00:01\r",
      "\u001b[K    51% |████████████████▋               | 512kB 50.6MB/s eta 0:00:01\r",
      "\u001b[K    52% |█████████████████               | 522kB 51.7MB/s eta 0:00:01\r",
      "\u001b[K    53% |█████████████████▎              | 532kB 53.7MB/s eta 0:00:01\r",
      "\u001b[K    54% |█████████████████▋              | 542kB 54.6MB/s eta 0:00:01\r",
      "\u001b[K    55% |██████████████████              | 552kB 63.8MB/s eta 0:00:01\r",
      "\u001b[K    57% |██████████████████▎             | 563kB 66.9MB/s eta 0:00:01\r",
      "\u001b[K    58% |██████████████████▋             | 573kB 67.3MB/s eta 0:00:01\r",
      "\u001b[K    59% |███████████████████             | 583kB 68.1MB/s eta 0:00:01\r",
      "\u001b[K    60% |███████████████████▎            | 593kB 68.4MB/s eta 0:00:01\r",
      "\u001b[K    61% |███████████████████▋            | 604kB 68.0MB/s eta 0:00:01\r",
      "\u001b[K    62% |████████████████████            | 614kB 78.0MB/s eta 0:00:01\r",
      "\u001b[K    63% |████████████████████▎           | 624kB 78.8MB/s eta 0:00:01\r",
      "\u001b[K    64% |████████████████████▋           | 634kB 80.5MB/s eta 0:00:01\r",
      "\u001b[K    65% |█████████████████████           | 645kB 82.6MB/s eta 0:00:01\r",
      "\u001b[K    66% |█████████████████████▎          | 655kB 81.7MB/s eta 0:00:01\r",
      "\u001b[K    67% |█████████████████████▋          | 665kB 53.6MB/s eta 0:00:01\r",
      "\u001b[K    68% |██████████████████████          | 675kB 52.9MB/s eta 0:00:01\r",
      "\u001b[K    69% |██████████████████████▎         | 686kB 52.8MB/s eta 0:00:01\r",
      "\u001b[K    70% |██████████████████████▋         | 696kB 52.8MB/s eta 0:00:01\r",
      "\u001b[K    71% |███████████████████████         | 706kB 52.7MB/s eta 0:00:01\r",
      "\u001b[K    72% |███████████████████████▎        | 716kB 52.5MB/s eta 0:00:01\r",
      "\u001b[K    73% |███████████████████████▋        | 727kB 52.8MB/s eta 0:00:01\r",
      "\u001b[K    74% |████████████████████████        | 737kB 52.1MB/s eta 0:00:01\r",
      "\u001b[K    75% |████████████████████████▎       | 747kB 50.9MB/s eta 0:00:01\r",
      "\u001b[K    76% |████████████████████████▋       | 757kB 50.6MB/s eta 0:00:01\r",
      "\u001b[K    77% |████████████████████████▉       | 768kB 75.1MB/s eta 0:00:01\r",
      "\u001b[K    78% |█████████████████████████▏      | 778kB 76.6MB/s eta 0:00:01\r",
      "\u001b[K    79% |█████████████████████████▌      | 788kB 75.4MB/s eta 0:00:01\r",
      "\u001b[K    80% |█████████████████████████▉      | 798kB 74.8MB/s eta 0:00:01\r",
      "\u001b[K    81% |██████████████████████████▏     | 808kB 53.9MB/s eta 0:00:01\r",
      "\u001b[K    82% |██████████████████████████▌     | 819kB 52.5MB/s eta 0:00:01\r",
      "\u001b[K    83% |██████████████████████████▉     | 829kB 52.2MB/s eta 0:00:01\r",
      "\u001b[K    85% |███████████████████████████▏    | 839kB 51.7MB/s eta 0:00:01\r",
      "\u001b[K    86% |███████████████████████████▌    | 849kB 51.7MB/s eta 0:00:01\r",
      "\u001b[K    87% |███████████████████████████▉    | 860kB 46.1MB/s eta 0:00:01\r",
      "\u001b[K    88% |████████████████████████████▏   | 870kB 44.4MB/s eta 0:00:01\r",
      "\u001b[K    89% |████████████████████████████▌   | 880kB 44.7MB/s eta 0:00:01\r",
      "\u001b[K    90% |████████████████████████████▉   | 890kB 45.0MB/s eta 0:00:01\r",
      "\u001b[K    91% |█████████████████████████████▏  | 901kB 45.2MB/s eta 0:00:01\r",
      "\u001b[K    92% |█████████████████████████████▌  | 911kB 59.5MB/s eta 0:00:01\r",
      "\u001b[K    93% |█████████████████████████████▉  | 921kB 62.3MB/s eta 0:00:01\r",
      "\u001b[K    94% |██████████████████████████████▏ | 931kB 63.3MB/s eta 0:00:01\r",
      "\u001b[K    95% |██████████████████████████████▌ | 942kB 64.6MB/s eta 0:00:01\r",
      "\u001b[K    96% |██████████████████████████████▉ | 952kB 65.8MB/s eta 0:00:01\r",
      "\u001b[K    97% |███████████████████████████████▏| 962kB 79.1MB/s eta 0:00:01\r",
      "\u001b[K    98% |███████████████████████████████▌| 972kB 83.8MB/s eta 0:00:01\r",
      "\u001b[K    99% |███████████████████████████████▉| 983kB 83.2MB/s eta 0:00:01\r",
      "\u001b[K    100% |████████████████████████████████| 993kB 21.2MB/s \n",
      "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Please comment your code\n",
    "# Please comment your code\n",
    "\n",
    "# Code to download personality files into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '1T-HcbBOf98KwUZcLuvbFUOepLqZhVHA-'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('qna_chitchat_the_comic.tsv') \n",
    "\n",
    "id = '11ykY78Tsagf9BAH_R2EgrAmz02iTLCw0'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('qna_chitchat_the_friend.tsv') \n",
    "\n",
    "id = '1stmouBrMsIoyDgPN5RhVy4qARwAJ3jhg'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('qna_chitchat_the_professional.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Fz6GoTQQNTl"
   },
   "outputs": [],
   "source": [
    "df_prof = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")\n",
    "df_comic = pd.read_csv('qna_chitchat_the_comic.tsv', sep=\"\\t\")\n",
    "df_friend = pd.read_csv('qna_chitchat_the_friend.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9gBSgBCQh24"
   },
   "source": [
    "## 1.2. Preprocess data (Personality chat datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RdKI8E2KRwe"
   },
   "source": [
    "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wHsl31Q6yJe"
   },
   "source": [
    "Personality chat datasets have lot of contractions and punctuation marks(e.g. ?) and plural forms. These have been removed. \n",
    "I haven't removed stop words from the datasets, as some stopwords like I, You etc. can be beneficial for dialog system.\n",
    "Finally these preprocessed words are tokenized and decapitalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emyl1lWxGr12"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# These are just common English contractions. There are some of those.\n",
    "\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_personality_bot(sentence):\n",
    "  \n",
    "  contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "  def expand_contractions(s, contraction_dict=contraction_dict):\n",
    "    def replace(match):\n",
    "      return contraction_dict[match.group(0)]\n",
    "    return contraction_re.sub(replace, s)\n",
    "\n",
    "\n",
    "  def remove_punctuation(s):\n",
    "    s = str(s)\n",
    "    for punct in puncts:\n",
    "        if punct in s:\n",
    "            s = s.replace(punct, '')\n",
    "\n",
    "    return s\n",
    "\n",
    "  def remove_lemma(s):\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    token_wo_lemma = []\n",
    "    for w in tokens:\n",
    "        token_wo_lemma.append(WordNetLemmatizer().lemmatize(w))\n",
    "    return token_wo_lemma\n",
    "\n",
    "\n",
    "  prep_sentence = sentence.apply(lambda x:expand_contractions((x.lower())))\n",
    "  prep_sentence = prep_sentence.apply(lambda x: remove_punctuation(x))\n",
    "  prep_sentence = prep_sentence.apply(lambda x:remove_lemma((x)))\n",
    "  \n",
    "  return prep_sentence\n",
    "\n",
    "\n",
    "prof_answer = df_prof.Answer\n",
    "comic_answer = df_comic.Answer\n",
    "friend_answer = df_friend.Answer\n",
    "\n",
    "input_df = pd.DataFrame()\n",
    "input_df['proc_question'] = preprocess_personality_bot(df_prof.Question)\n",
    "input_df['proc_prof_answer'] = preprocess_personality_bot(df_prof.Answer)\n",
    "input_df['proc_comic_answer'] = preprocess_personality_bot(df_comic.Answer)\n",
    "input_df['proc_friend_answer'] = preprocess_personality_bot(df_friend.Answer)\n",
    "\n",
    "input_df['prof_answer'] = df_prof.Answer\n",
    "input_df['comic_answer'] = df_comic.Answer\n",
    "input_df['friend_answer'] = df_friend.Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIu_lkJwQ55g"
   },
   "source": [
    "# 2 - Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "daDvAftceIvr"
   },
   "source": [
    "## 2.1. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbzm-NWBTmM-"
   },
   "source": [
    "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T5duj1ov58o"
   },
   "source": [
    "I have used gensim word2vec with CBOW model to train the model, because it is faster.\n",
    "Our corpus is a twitter corpus, which does have a lot of infrequent words.\n",
    "So, skip gram wasn't necessary.\n",
    "Also, FastText does not show model loss, that was another reason for not choosing fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "it6I1_K7HTub"
   },
   "source": [
    "### 2.1.1. Download Dataset for Word Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Op66omXKVHa"
   },
   "source": [
    "*You are required to describe which data was used with justification of your decision.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxBe3leWwMcU"
   },
   "source": [
    "Personality datasets are small in size and hence not suitable for training.\n",
    "I chose twitter_en.gz dataset as it would contain modern linguals and vocabulary,  and I am going to use similar languals to chat with the bot.\n",
    "link -- https://github.com/Marsan-Ma/chat_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLjf_pm9NiA8"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "id = '1Fl6bPfe_NUqiShOFaZkU7KVN4xn4cRRS'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('twitter.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXgFpxIgl-_G"
   },
   "source": [
    "### 2.1.2. Data Preprocessing for Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJrVHGYSmYMg"
   },
   "source": [
    "*You are required to describe which preprocessing techniques were used with justification of your decision.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d910SsZ_wanr"
   },
   "source": [
    "Instead of using my own preprocessing function(written in section 1.2), I used gensim pre-process function. \n",
    "Gensim preprocess step can process the large corpus in reasonable  time.\n",
    "This preprocessing step tokenizes, decapitalizes and removes punctionation and does not remove stop words, which are important for dialog system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LByzHLiNinu"
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with gzip.open ('twitter.gz', 'rb') as f:\n",
    "        for i,line in enumerate (f):\n",
    "            preprocessed_line = gensim.utils.simple_preprocess(line)\n",
    "            corpus.append(preprocessed_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhAgWf_AmbZ8"
   },
   "source": [
    "### 2.1.3. Build Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJ8rU7JbiBVS"
   },
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*\n",
    "\n",
    "I chose size of embedding to be decently large (150) and \n",
    "learning rate 0.03 steadily reduced loss after every iteration.\n",
    "with smaller learning rate, decrease in loss was slow.\n",
    "Average length of a sentence in my corpus is 15, that's why I chose window size 10.\n",
    "After 10 iterations, loss was reduced by 33% of it's initial value and \n",
    "it was able to provide similarity of two words quite well. SO, I stopped at 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_r_9aGnlGCk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    def __init__(self, savedir):\n",
    "        self.savedir = savedir\n",
    "        self.epoch = 0\n",
    "\n",
    "        os.makedirs(self.savedir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        savepath = os.path.join(self.savedir, \"w2vmodel_{}_epoch.gz\".\\\n",
    "                                format(self.epoch))\n",
    "        model.save(savepath)\n",
    "        print(\n",
    "            \"Epoch saved: {}\".format(self.epoch + 1),\n",
    "            \"Start next epoch ... \", sep=\"\\n\"\n",
    "            )\n",
    "        if os.path.isfile(os.path.join(self.savedir, \"w2vmodel_{}_epoch.gz\".\\\n",
    "                                       format(self.epoch - 1))):\n",
    "            print(\"Previous model deleted \")\n",
    "            os.remove(os.path.join(self.savedir, \"w2vmodel_{}_epoch.gz\".\\\n",
    "                                   format(self.epoch - 1))) \n",
    "        self.epoch += 1\n",
    "        print(\"Word2Vec Model loss:\", model.get_latest_training_loss())\n",
    "        \n",
    "def train_word2vec(iter):\n",
    "\n",
    "    ### Initialize model ###\n",
    "    print(\"Start training Word2Vec model\")\n",
    "    corpus_list = corpus\n",
    "    w2vmodel = gensim.models.Word2Vec(\n",
    "        corpus_list,\n",
    "        size=150, alpha=0.03, min_alpha=0.00025, iter=iter,\n",
    "        min_count=10, hs=0, negative=10, workers=10,\n",
    "        window=10, callbacks=[EpochSaver(\"./\")], \n",
    "        compute_loss=True\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LNys5HOdISK-"
   },
   "source": [
    "### 2.1.4. Train Word Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "colab_type": "code",
    "id": "Ae8i7Z2kIef-",
    "outputId": "e1b94822-d698-49a5-9063-4045ace8532c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training Word2Vec model\n",
      "Epoch saved: 1\n",
      "Start next epoch ... \n",
      "Word2Vec Model loss: 2177288.0\n",
      "Epoch saved: 2\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 4040886.0\n",
      "Epoch saved: 3\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 5681168.0\n",
      "Epoch saved: 4\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 7266022.0\n",
      "Epoch saved: 5\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 8780709.0\n",
      "Epoch saved: 6\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 10216491.0\n",
      "Epoch saved: 7\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 11601287.0\n",
      "Epoch saved: 8\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 12959488.0\n",
      "Epoch saved: 9\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 14307771.0\n",
      "Epoch saved: 10\n",
      "Start next epoch ... \n",
      "Previous model deleted \n",
      "Word2Vec Model loss: 15626345.0\n"
     ]
    }
   ],
   "source": [
    "# Please comment your code\n",
    "# train word2vec model\n",
    "iter = 10\n",
    "train_word2vec(iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNadw-85a-H_"
   },
   "source": [
    "By looking at the loss value, it seems like loss is increasing after getting trained. But this is not as it seems.\n",
    "This is an representation problem of gensim -->\n",
    "https://github.com/RaRe-Technologies/gensim/pull/2135\n",
    "\n",
    "In reality, Loss value after ith iteration = (loss  after ith iteration )- (loss after i-1th iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMCv3YI1IfUo"
   },
   "source": [
    "### 2.1.5. Save Word Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OwicNPkIqd1"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "'''\n",
    "Please see 2.1.3\n",
    "I have a block of code in model build that saves the model at the end of\n",
    "every epoch\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yn16xrDrIs8B"
   },
   "source": [
    "### 2.1.6. Load Word Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IebpYFsIvgh"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# restore the model\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    def __init__(self, savedir):\n",
    "        self.savedir = savedir\n",
    "        self.epoch = 0\n",
    "\n",
    "        os.makedirs(self.savedir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        savepath = os.path.join(self.savedir, \"w2vmodel_{}_epoch.gz\".\\\n",
    "                                format(self.epoch))\n",
    "        model.save(savepath)\n",
    "        print(\n",
    "            \"Epoch saved: {}\".format(self.epoch + 1),\n",
    "            \"Start next epoch ... \", sep=\"\\n\"\n",
    "            )\n",
    "        if os.path.isfile(os.path.join(self.savedir, \"w2vmodel_{}_epoch.gz\".\\\n",
    "                                       format(self.epoch - 1))):\n",
    "            print(\"Previous model deleted \")\n",
    "            os.remove(os.path.join(self.savedir, \"w2vmodel_{}_epoch.gz\".\\\n",
    "                                   format(self.epoch - 1))) \n",
    "        self.epoch += 1\n",
    "        print(\"Word2Vec Model loss:\", model.get_latest_training_loss())\n",
    "\n",
    "w2v_saved_model = gensim.models.Word2Vec.load('./w2vmodel_9_epoch.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSh-f7r1fvSU"
   },
   "outputs": [],
   "source": [
    "index2word_set = set(w2v_saved_model.wv.index2word)\n",
    "\n",
    "# this function assigns one vector for each answer of personality dataset.\n",
    "# sentence --> Question\n",
    "# w2v_saved_model --> the restored model\n",
    "# num_features --> size of each vector\n",
    "# index2word_set --> index and word mapping of the corpus\n",
    "# returns a vector of size 150\n",
    "\n",
    "def avg_feature_vector(sentence, w2v_saved_model, num_features, index2word_set):\n",
    "\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in sentence:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, w2v_saved_model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlCeWT8eeLnd"
   },
   "source": [
    "## 2.2. Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwA-NN3EJ4Ig"
   },
   "source": [
    "### 2.2.1. Apply/Import Word Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAMJrxx-iOVn"
   },
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g7PKX1gIePA2"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# this function returns one-hot encoded question vector and answer vector\n",
    "# seq_data --> list of Question vector, answer vector tuple\n",
    "# num_dic --> dictionary made from all the unique words in one personality dataset\n",
    "# max_input_words_amount --> returns number of tokens/vectors of the question set\n",
    "\n",
    "def make_batch(seq_data, num_dic, max_input_words_amount):\n",
    "  input_batch = []\n",
    "  output_batch = []\n",
    "  target_batch = []\n",
    "  \n",
    "  for seq in seq_data: \n",
    "      # Input for encoder cell, convert question to vector\n",
    "\n",
    "      input_data = get_vectors_q(seq[0], num_dic, max_input_words_amount)  \n",
    "\n",
    "\n",
    "      # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
    "\n",
    "      output_data = [num_dic[str(['_B_'])]]\n",
    "      output_data += get_vectors_a(seq[1], num_dic)\n",
    "\n",
    "      target = [num_dic[str(seq[1])]]\n",
    "      target.append(num_dic[str(['_E_'])])\n",
    "      dic_len = len(num_dic)\n",
    "\n",
    "      # Convert each token vector to one-hot encode data\n",
    "      input_batch.append(np.eye(dic_len)[input_data])\n",
    "      output_batch.append(np.eye(dic_len)[output_data])\n",
    "      target_batch.append(target)\n",
    "\n",
    "  return input_batch, output_batch, target_batch\n",
    "\n",
    "\n",
    "# This function is called to make sure program does not fail\n",
    "# when input is not in corpus. when input is not in vocabulary it returns a \n",
    "# list of 150(dimension of vector) zeroes.\n",
    "\n",
    "def catch_NOV_error(func, handle=lambda e : e, *args, **kwargs):\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        return np.zeros(shape=(150,)).tolist()\n",
    "\n",
    "# This function is called 3 times each for one personality(answer_type).\n",
    "# based on the answer_type, function cooses the answer set and applies \n",
    "# word embedding on questions and answers.\n",
    "# This function returns below parameters.\n",
    "\n",
    "# unique_words--> unique vectors in the personality dataset. \n",
    "# Each vocabulary of question gets a unique vector, each answer gets a unique vector\n",
    "# We get answer vector by calling avg_feature_vector()\n",
    "# answer_lookup --> list of all the answer vectors. \n",
    "# Will be used to convert the vector to original answer\n",
    "# num_dic --> dictionary made from unique_words, where each key is the \n",
    "# string representation of vector\n",
    "\n",
    "def prep_data_for_batch(answer_type):\n",
    "  seq_data = []\n",
    "  answer_lookup = []\n",
    "  unique_words = []\n",
    "  max_input_words_amount = 0\n",
    "  for index, row in input_df.iterrows():\n",
    "      tokenized_q = row.proc_question\n",
    "            \n",
    "      # we need to tokenise question    \n",
    "      question_vector = [catch_NOV_error(lambda : w2v_saved_model.wv[i].\\\n",
    "                                         tolist()) for i in tokenized_q]\n",
    "      \n",
    "      if answer_type == 'Friend':\n",
    "        answer_row = row.proc_friend_answer\n",
    "      elif answer_type == 'Comic':\n",
    "        answer_row = row.proc_comic_answer\n",
    "      else:\n",
    "        answer_row = row.proc_prof_answer\n",
    "      \n",
    "      answer_vector = avg_feature_vector(answer_row, w2v_saved_model, 150, index2word_set)\n",
    "      \n",
    "      seq_data.append([question_vector, answer_vector])\n",
    "      answer_lookup.append(answer_vector)\n",
    "      # add question list and answer list (one element)\n",
    "      for item in question_vector:\n",
    "          if item not in unique_words:\n",
    "              unique_words.append(item)\n",
    "\n",
    "      if answer_vector not in unique_words:\n",
    "          unique_words.append(answer_vector)\n",
    "\n",
    "      # we need to decide the maximum size of input word tokens\n",
    "      max_input_words_amount = max(len(question_vector), max_input_words_amount)\n",
    "\n",
    "# adding special tokens in the vocabulary list    \n",
    "# _B_: Beginning of Sequence\n",
    "# _E_: Ending of Sequence\n",
    "# _P_: Padding of Sequence - for different size input\n",
    "# _U_: Unknown element of Sequence - for different size input\n",
    "\n",
    "  unique_words = [str(item) for item in unique_words]\n",
    "\n",
    "  unique_words.append(repr(['_B_']))\n",
    "  unique_words.append(repr(['_E_']))\n",
    "  unique_words.append(repr(['_P_']))\n",
    "  unique_words.append(repr(['_U_']))\n",
    "\n",
    "  num_dic = {n: i for i, n in enumerate(unique_words)}\n",
    "  dic_len = len(num_dic)\n",
    "  \n",
    "  \n",
    "### Neural Network Model\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  return unique_words, answer_lookup, seq_data, dic_len, num_dic, max_input_words_amount\n",
    "\n",
    "# get token index vector of questions and add paddings \n",
    "# if the word is shorter than the maximum number of words\n",
    "def get_vectors_q(sentence, num_dic, max_input_words_amount):\n",
    "  diff = max_input_words_amount - len(sentence)\n",
    "  # add paddings if the word(vector) is shorter than the maximum number of words    \n",
    "  for x in range(diff):\n",
    "      sentence.append(['_P_'])    \n",
    "  data = tokens_to_ids(sentence, num_dic)       \n",
    "  return data\n",
    "\n",
    "# get token index vector of answer\n",
    "def get_vectors_a(vector, num_dic):    \n",
    "  ids = []\n",
    "  if str(vector) in num_dic:\n",
    "      ids.append(num_dic[str(vector)])\n",
    "  else:\n",
    "      ids.append(num_dic[str(['_U_'])])\n",
    "  data = ids\n",
    "  return data\n",
    "\n",
    "\n",
    " # convert tokens to index\n",
    "def tokens_to_ids(vector_list, num_dic):\n",
    "  ids = [] \n",
    "  for vector in vector_list:\n",
    "      if str(vector) in num_dic:\n",
    "          ids.append(num_dic[str(vector)])\n",
    "      else:\n",
    "          ids.append(num_dic[str(['_U_'])])\n",
    "  return ids\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpYCL17JKZxl"
   },
   "source": [
    "### 2.2.2. Build Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R204UIyDKhZ4"
   },
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNsCJJkAxFk5"
   },
   "source": [
    "This functions build, train and save the seq2seq model for each personality.\n",
    "Hyper-parameter 0.01 reduces training loss quite faster, \n",
    "within first 2 epoch, loss reduces from 6 to 2. \n",
    "These seq2seq models are huge in size, so I used epoch(300) only\n",
    "and got optimal result(loss = 2) by choosing learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13eCtR_SLUG6"
   },
   "outputs": [],
   "source": [
    "# # Please comment your code\n",
    "\n",
    "def build_train_save_s2s(personality):\n",
    "\n",
    "  seq_data = []\n",
    "  ### Setting Hyperparameters\n",
    "  unique_words, answer_lookup, seq_data, dic_len, num_dic,\\\n",
    "                       max_input_words_amount = prep_data_for_batch(personality)\n",
    "  learning_rate = 0.01\n",
    "  n_hidden = 128\n",
    "\n",
    "  n_class = dic_len\n",
    "  n_input = dic_len\n",
    "\n",
    "  ### Neural Network Model\n",
    "  tf.reset_default_graph()\n",
    "  \n",
    "  # encoder/decoder shape = [batch size, time steps, input size]\n",
    "  enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "  dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "\n",
    "  # target shape = [batch size, time steps]\n",
    "  targets = tf.placeholder(tf.int64, [None, None])\n",
    "  \n",
    "  # Encoder Cell\n",
    "#   with g1.as_default() as g:\n",
    "  with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
    "                                            dtype=tf.float32)\n",
    "  # Decoder Cell\n",
    "  with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "\n",
    "    # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=enc_states,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "  seq2seq_model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "  cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=seq2seq_model, labels=targets))\n",
    "\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "  ### Training Model\n",
    "  saver1 = tf.train.Saver(reshape=True)\n",
    "  \n",
    "  sess1 = tf.Session()\n",
    "  sess1.run(tf.initialize_all_variables())\n",
    "  \n",
    "  # Generate a batch data\n",
    "  input_batch, output_batch, target_batch= make_batch(seq_data, num_dic, max_input_words_amount)\n",
    "\n",
    "  total_epoch = 300\n",
    "\n",
    "  for epoch in range(total_epoch):\n",
    "      _, loss = sess1.run([optimizer, cost],\n",
    "                         feed_dict={enc_input: input_batch,\n",
    "                                    dec_input: output_batch,\n",
    "                                    targets: target_batch})\n",
    "      if epoch % 100 == 0:\n",
    "          print('Epoch:', '%04d' % (epoch + 1),\n",
    "                'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "  print('Epoch:', '%04d' % (epoch + 1),\n",
    "        'cost =', '{:.6f}'.format(loss))\n",
    "  print('Training completed')\n",
    "  tf.add_to_collection('seq2seq_model', seq2seq_model)\n",
    "  tf.add_to_collection('enc_input', enc_input)\n",
    "  tf.add_to_collection('dec_input', dec_input)\n",
    "  tf.add_to_collection('targets', targets)\n",
    "\n",
    "  saver1.save(sess1, './s2s_{}_model_new'.format(personality))\n",
    "  sess1.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BaOiaGRLW7R"
   },
   "source": [
    "### 2.2.3. Train Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "HjuwPSpVO605",
    "outputId": "4780a53a-bd23-4b30-ae85-90199370932b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-80769b02592f>:25: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-15-80769b02592f>:29: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-15-80769b02592f>:40: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch: 0001 cost = 6.357491\n",
      "Epoch: 0101 cost = 2.279769\n",
      "Epoch: 0201 cost = 2.288945\n",
      "Epoch: 0300 cost = 2.370597\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "build_train_save_s2s('Professional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "EGcdxuqQ1iyS",
    "outputId": "933073da-6c03-4e75-d164-4db8334f8cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 6.312863\n",
      "Epoch: 0101 cost = 2.271080\n",
      "Epoch: 0201 cost = 2.037329\n",
      "Epoch: 0300 cost = 1.988876\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "build_train_save_s2s('Friend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "qC0ZUzJ01iLY",
    "outputId": "512409de-4268-4419-f2bc-e0ebebc38bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 6.330794\n",
      "Epoch: 0101 cost = 2.654783\n",
      "Epoch: 0201 cost = 4.134859\n",
      "Epoch: 0300 cost = 2.351503\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "build_train_save_s2s('Comic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2feNpG-LZx2"
   },
   "source": [
    "### 2.2.4. Save Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsQIm7_-1he3"
   },
   "outputs": [],
   "source": [
    "# models were saved while training in section 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDfYybWoC5HL"
   },
   "outputs": [],
   "source": [
    "# downlaod the models to save in google drive\n",
    "files.download('w2vmodel_9_epoch.gz')\n",
    "s2s_files = [files.download(f) for f in os.listdir('.') if re.match(r's2s*', f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zFo6YppL6w3"
   },
   "source": [
    "### 2.2.5. Load Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZzLt6EkOh1c"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# This function loads the saved s2s models and get the saved tensors \n",
    "# then, it calls prep_data_for_batch to get unique_words, answer_lookup, num_dic \n",
    "# for that personality\n",
    "# Finally it calls response() method to get the answer.\n",
    "def answer(sentence, conv_type):\n",
    "  tf.reset_default_graph()\n",
    "  if conv_type == 'Friend':\n",
    "    model_name = 'Friend'\n",
    "  elif conv_type == 'Comic':\n",
    "    model_name = 'Comic'\n",
    "  else:\n",
    "    model_name = 'Professional'\n",
    "  \n",
    "  answer_lookup = []\n",
    "  unique_words = []\n",
    "  num_dic = {}\n",
    "  max_input_words_amount = 0\n",
    "  saver = tf.train.import_meta_graph(\"./s2s_{}_model_new.meta\".format(model_name))\n",
    "  sess =  tf.Session()\n",
    "\n",
    " \n",
    "  # Restore (Load) the model\n",
    "  saver.restore(sess, \"./s2s_{}_model_new\".format(model_name))\n",
    "  \n",
    "  enc_input = tf.get_collection('enc_input')[0]\n",
    "  dec_input = tf.get_collection('dec_input')[0]\n",
    "  targets = tf.get_collection('targets')[0]\n",
    "  seq2seq_model = tf.get_collection('seq2seq_model')[0]\n",
    "\n",
    "  unique_words, answer_lookup, seq_data, dic_len, num_dic, max_input_words_amount = prep_data_for_batch(model_name)\n",
    "  \n",
    "  bot_response = response(sentence, conv_type, sess, unique_words, answer_lookup, num_dic, seq2seq_model, enc_input, dec_input, targets, max_input_words_amount )\n",
    "  \n",
    "  sess.close()\n",
    "  return bot_response\n",
    "\n",
    "\n",
    "# This function runs the restored model to get the prediction of user input\n",
    "# and returns the answer of user question\n",
    "# sentence --> list of question vector and answer vector tuple, \n",
    "# Since, we do not know the answer at the beginning, answer vector is '_U_'\n",
    "\n",
    "def response(sentence, conv_type, sess,  unique_words, answer_lookup, num_dic, seq2seq_model, enc_input, dec_input, targets, max_input_words_amount):\n",
    "  answer_index = 0\n",
    "  seq_data = [sentence, ['_U_']]\n",
    "\n",
    "  input_batch, output_batch, target_batch = make_batch([seq_data], num_dic, max_input_words_amount)\n",
    "\n",
    "  prediction = tf.argmax(seq2seq_model, 2)\n",
    "\n",
    "  result = sess.run(prediction,\n",
    "                    feed_dict={enc_input: input_batch,\n",
    "                               dec_input: output_batch,\n",
    "                               targets: target_batch})\n",
    "\n",
    "  \n",
    "  decoded = [unique_words[i] for i in result[0]]\n",
    "  # in case, program malfunctions and not able to find the answer \n",
    "  # in answer vector list, instead of failing, it will return 'Oops'\n",
    "  try:\n",
    "    answer_index = (answer_lookup.index(eval(decoded[0])))\n",
    "  except:\n",
    "    response = \"Oops\"\n",
    "  \n",
    "  if conv_type == 'Friend':\n",
    "    col_name = 'friend_answer'\n",
    "  elif conv_type == 'Comic':\n",
    "    col_name = 'comic_answer'\n",
    "  else:\n",
    "    col_name = 'prof_answer'\n",
    "    \n",
    "  # gets the answer by looking up the personality datadrame by answer_index\n",
    "  response = input_df.iloc[answer_index][col_name]\n",
    "\n",
    "  return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4mpRpocePLN"
   },
   "source": [
    "# 3 - Evaluation (Running chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEW1zMgVMREr"
   },
   "source": [
    "## 3.1. Start chatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P28Z1k36MZuo"
   },
   "source": [
    "## 3.2. Change Personality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8OBtJfvMgL_"
   },
   "source": [
    "*Explain how to change personality (What is the command for changing personality?). *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqPJK_6UvNwg"
   },
   "source": [
    "User can change the personality by entering command 'change personality'\n",
    "Bot then asks for user's choice of personality\n",
    "User can enter 'Friend or 'Comic' to enter these personality modes,\n",
    "Any other input will bring him to professional bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTLyQEeZMZ2f"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "\n",
    "\n",
    "def change_personality(input):\n",
    "  if input == 'Friend':\n",
    "    print('Hey Friend')\n",
    "    personality = 'Friend'\n",
    "  elif input == 'Comic':\n",
    "    print('I am funny, talk to me')\n",
    "    personality = 'Comic'\n",
    "  else:\n",
    "    print(\"Let's talk business\")\n",
    "    personality = 'Professional'\n",
    "  return personality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y50Ep8KKMZ99"
   },
   "source": [
    "## 3.3. Save chat log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGlsNTNHu_pg"
   },
   "source": [
    "Once user enters end chat command 'end_conv',\n",
    "Bot gives the option of saving chat file. If user says Y, \n",
    "The chat log is automatically saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hwNz-nkcOlSE"
   },
   "outputs": [],
   "source": [
    "# supress showing information like 'model is restored'\n",
    "import logging\n",
    "import logging.config\n",
    "logging.config.dictConfig({\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': True,\n",
    "})\n",
    "\n",
    "logger = logging.getLogger(\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbZ6oOu6MaGJ"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# This function takes input from user and generates bot response\n",
    "# At the end of the conversation user can choose to save the chat log\n",
    "\n",
    "def call_bot():\n",
    "  if os.path.exists('chat_log.txt'):\n",
    "    os.remove('chat_log.txt')\n",
    "  personality = 'Professional'\n",
    "  user_sentence = 'start chat'\n",
    "  while user_sentence!= 'end_conv':\n",
    "    user_sentence = input('User:')\n",
    "    # default personality is professional\n",
    "    if user_sentence == 'change personality':\n",
    "      print('Give me your choice')\n",
    "      personality_input = input(\"User:\")\n",
    "      personality = change_personality(personality_input)\n",
    "      user_sentence = input(\"User:\")\n",
    "    if user_sentence == 'end_conv':\n",
    "      print(\"Press Y if you want a chat log\")\n",
    "      user_sentence = input('User:')\n",
    "      if user_sentence == 'Y':\n",
    "        files.download('chat_log.txt')\n",
    "      print(\"Good talk, bye for now\")\n",
    "      break  \n",
    "    my_tokenized_q = nltk.word_tokenize(user_sentence)\n",
    "    my_question_vectors =[catch_NOV_error(lambda:w2v_saved_model.wv[i.lower()].\\\n",
    "                                          tolist()) for i in my_tokenized_q]\n",
    "    # calls answer() to run the restored model and get the answer\n",
    "    bot_answer = answer(my_question_vectors, personality)\n",
    "    print('Bot:', bot_answer)\n",
    "    with open(\"chat_log.txt\", \"a\") as f:\n",
    "      f.write(\"{} --> {}\".format(user_sentence, bot_answer) + \"\\r\\n\")\n",
    "      \n",
    "      \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JISqR3jjMwwU"
   },
   "source": [
    "## 3.4. End chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nT_DeoHSMw49"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# In 3.3. section, end chat command is encapsulated in the function\n",
    "# end chat command is 'end_conv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpomO_3YNI5X"
   },
   "source": [
    "## 3.5. Execute program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDkQJ9i_NH9D"
   },
   "source": [
    "***Please make sure your program  is running properly.***\n",
    "\n",
    "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NV0J9BCHoSGw"
   },
   "source": [
    "Instruction for execution:\n",
    "1. Execute 1.1. section to execute import statements\n",
    "2. Execute 1.2 section to pre-process personality datasets\n",
    "3. Execute 3.5 section to restore the models\n",
    "4. Execute 2.1.6 to restore word2vec model\n",
    "5. Execute 2.2.1 which defines the function to apply word2vec model\n",
    "6. Execute 2.2.5 to load the seq2seq models\n",
    "7. Execute 3.2 to define change personality function\n",
    "8. Execute 3.3 to define chat function\n",
    "9. Execute 3.5.2 to start chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBKJzeCEqF6X"
   },
   "outputs": [],
   "source": [
    "# Code to download saved models into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '1aHHX9m68AmqmPl4hsKSfhjL3QwLXId0L'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Comic_model_new.data-00000-of-00001')\n",
    "\n",
    "id = '1OTWwxWHaW7_XuxTQpCuOZFdLNNcrvyQo'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Comic_model_new.index')\n",
    "\n",
    "id = '1-MgEsDUGMQaqLwdGbXOD6zuFnQzSi-k3'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Comic_model_new.meta')\n",
    "\n",
    "id = '1Q9XVc05BTHE9Mi7HdYnEL-_V6ddftWjF'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Professional_model_new.data-00000-of-00001')\n",
    "\n",
    "id = '1jDVrOqe5buIYjCRSGsnQQdmeli5xB9_Q'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Professional_model_new.index')\n",
    "\n",
    "id = '1XkT2TEKVMEE7EhgCArQNZd_Pt_htb3pu'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Professional_model_new.meta')\n",
    "\n",
    "id = '1l90mP9oIaeSN8fvqS4IRewHD7RDlbjes'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Friend_model_new.data-00000-of-00001')\n",
    "\n",
    "id = '1NHFSEz65rVNQTOSqraMnK0s41r0F_jy-'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Friend_model_new.index')\n",
    "\n",
    "id = '1M2XhYWN7v7BLD4QmDV5dHz_Zd5tA6eAs'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('s2s_Friend_model_new.meta')\n",
    "\n",
    "id = '1XivBd-pvHo4Cfendlg7CpBLGqbPNaFdD'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('w2vmodel_9_epoch.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7J5hS_SOIUU"
   },
   "source": [
    "### 3.5.1. Execute program - training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_woLwuU3Mk3w"
   },
   "source": [
    "*Please include lines to train the bot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhWYz7NQOfLV"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "\n",
    "# we can train seq2seq model by executing 2.2.2\n",
    "# we can train word2vec model by executing 2.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65cZTuQ_OeI7"
   },
   "source": [
    "### 3.5.2. Execute program - chatting mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7LrbcP_PKap"
   },
   "source": [
    "*Please include lines to start chatting with the bot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2010
    },
    "colab_type": "code",
    "id": "QVvzZsB7PbYf",
    "outputId": "0defb784-f867-4f6a-99f2-32195523470c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's start talking\n",
      "User:how are you\n",
      "Bot: I don't have family.\n",
      "User:are you happy\n",
      "Bot: I don't have family.\n",
      "User:do you know other bots\n",
      "Bot: That's a biological concept that doesn't apply to me.\n",
      "User:you are stupid\n",
      "Bot: That's not me, but hello.\n",
      "User:i miss you\n",
      "Bot: I don't have family.\n",
      "User:NLP assignment is over\n",
      "Bot: I don't have family.\n",
      "User:it was good\n",
      "Bot: I'm afraid I'm not musically inclined.\n",
      "User:you are sweet\n",
      "Bot: Let's keep things professional.\n",
      "User:you are giving wrong answers\n",
      "Bot: Goodbye.\n",
      "User:do you like me\n",
      "Bot: I don't have family.\n",
      "User:who is god\n",
      "Bot: I don't have family.\n",
      "User:life is short\n",
      "Bot: I hope you're able to get some rest soon.\n",
      "User:i hate my life\n",
      "Bot: I do what I can.\n",
      "User:i love to travel\n",
      "Bot: I aim for efficiency.\n",
      "User:i love my mother\n",
      "Bot: I'm afraid I'm not musically inclined.\n",
      "User:Do you have family\n",
      "Bot: I aim for efficiency.\n",
      "User:change personality\n",
      "Give me your choice\n",
      "User:Comic\n",
      "I am funny, talk to me\n",
      "User:how are you\n",
      "Bot: Those who can, do. Those who can't, don't sing.\n",
      "User:are you happy\n",
      "Bot: All those years at charm school. Wasted.\n",
      "User:do you know other bots\n",
      "Bot: You're looking at it.\n",
      "User:you are stupid\n",
      "Bot: OK. See you tomorrow.\n",
      "User:i miss you\n",
      "Bot: Hey.\n",
      "User:NLP assignment is over\n",
      "Bot: I'm age-free.\n",
      "User:it was good\n",
      "Bot: Those who can, do. Those who can't, don't sing.\n",
      "User:you are sweet\n",
      "Bot: Okay.\n",
      "User:you are giving wrong answers\n",
      "Bot: Sure, why not.\n",
      "User:do you like me\n",
      "Bot: Those who can, do. Those who can't, don't sing.\n",
      "User:who is god\n",
      "Bot: Not so far.\n",
      "User:life is short\n",
      "Bot: Yup.\n",
      "User:i hate my life\n",
      "Bot: I'm binary.\n",
      "User:i love to travel\n",
      "Bot: I'm age-free.\n",
      "User:i love my mother\n",
      "Bot: Sorry to hear that. Here's a virtual high five if that will help.\n",
      "User:Do you have family\n",
      "Bot: Not so far.\n",
      "User:change personality\n",
      "Give me your choice\n",
      "User:Friend\n",
      "Hey Friend\n",
      "User:how are you\n",
      "Bot: I'm doing great, thanks for asking!\n",
      "User:are you happy\n",
      "Bot: I have my moments.\n",
      "User:do you know other bots\n",
      "Bot: I haven't met any other bots, but I bet we'd get along.\n",
      "User:you are stupid\n",
      "Bot: I'm so sorry.\n",
      "User:i miss you\n",
      "Bot: I think I may have lost my train of thought.\n",
      "User:NLP assignment is over\n",
      "Bot: I don't have the hardware for that.\n",
      "User:it was good\n",
      "Bot: I'm so sorry to hear that! I'm happy to keep chatting if that will help.\n",
      "User:you are sweet\n",
      "Bot: Cool.\n",
      "User:you are giving wrong answers\n",
      "Bot: I'm so sorry.\n",
      "User:do you like me\n",
      "Bot: I don't have the hardware for that.\n",
      "User:who is god\n",
      "Bot: You're laughing!\n",
      "User:life is short\n",
      "Bot: I have many likes.\n",
      "User:i hate my life\n",
      "Bot: I haven't met any other bots, but I bet we'd get along.\n",
      "User:change personality\n",
      "Give me your choice\n",
      "User:Professional\n",
      "Let's talk business\n",
      "User:i hate my life\n",
      "Bot: I don't have family.\n",
      "User:change personality\n",
      "Give me your choice\n",
      "User:Comic\n",
      "I am funny, talk to me\n",
      "User:i miss you\n",
      "Bot: We're cool.\n",
      "User:end_conv\n",
      "Press Y if you want a chat log\n",
      "User:N\n",
      "Good talk, bye for now\n"
     ]
    }
   ],
   "source": [
    "# Please comment your code\n",
    "print(\"Let's start talking\")\n",
    "call_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfv8rWTKPzeb"
   },
   "source": [
    "## Object Oriented Programming codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TS23AjBRSZaX"
   },
   "source": [
    "*If you have multiple classes use multiple code snippets to add them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "caL7zlpa104F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "490166637_COMP5046_Ass1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
